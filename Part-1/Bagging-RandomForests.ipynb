{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q3"
      ],
      "metadata": {
        "id": "l9EIbtK5ezv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71eNOjgVseX7",
        "outputId": "c7225db8-d752-40da-c6a6-501cad4365c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Age  Income Student     Credit  Buy\n",
            "0   25    High      No       Fair   No\n",
            "1   30    High      No  Excellent   No\n",
            "2   35  Medium      No       Fair  Yes\n",
            "3   40     Low     Yes       Fair  Yes\n",
            "4   45     Low     Yes  Excellent  Yes\n",
            "5   50     Low      No  Excellent   No\n",
            "6   55  Medium      No  Excellent  Yes\n",
            "7   60    High     Yes       Fair   No\n",
            "Index(['Age', 'Income', 'Student', 'Credit', 'Buy'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "dataset = [\n",
        "    {\"Age\": 25, \"Income\": \"High\", \"Student\": \"No\", \"Credit\": \"Fair\", \"Buy\": \"No\"},\n",
        "    {\"Age\": 30, \"Income\": \"High\", \"Student\": \"No\",  \"Credit\": \"Excellent\", \"Buy\": \"No\"},\n",
        "    {\"Age\": 35, \"Income\": \"Medium\", \"Student\": \"No\",  \"Credit\": \"Fair\", \"Buy\": \"Yes\"},\n",
        "    {\"Age\": 40, \"Income\": \"Low\", \"Student\": \"Yes\", \"Credit\": \"Fair\", \"Buy\": \"Yes\"},\n",
        "    {\"Age\": 45, \"Income\": \"Low\", \"Student\": \"Yes\", \"Credit\": \"Excellent\", \"Buy\": \"Yes\"},\n",
        "    {\"Age\": 50, \"Income\": \"Low\", \"Student\": \"No\",  \"Credit\": \"Excellent\", \"Buy\": \"No\"},\n",
        "    {\"Age\": 55, \"Income\": \"Medium\", \"Student\": \"No\",  \"Credit\": \"Excellent\", \"Buy\": \"Yes\"},\n",
        "    {\"Age\": 60, \"Income\": \"High\", \"Student\": \"Yes\", \"Credit\": \"Fair\", \"Buy\": \"No\"}\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "print(df)                                             # just to verify the data\n",
        "\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gini Impurity\n",
        "As in class, the loss function Gini Index/Impurity is defined by:\n",
        "<br>\n",
        "G = Σ P mk(1 - P mk), summed over all classes k. This is the same as:\n",
        "<br>\n",
        "G = Σ P mk - Σ (P mk)^2 = 1 - Σ (P mk)^2.\n",
        "<br>\n",
        "I use this formula to compute the G."
      ],
      "metadata": {
        "id": "ljk-1E5fLrRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(labels):\n",
        "    \"\"\"\n",
        "    y: array-like of labels (e.g., 'Yes' or 'No')\n",
        "    Returns the Gini impurity of this set of labels.\n",
        "    \"\"\"\n",
        "    unique_labels, counts = np.unique(labels, return_counts = True)\n",
        "    probabilities = counts / counts.sum()\n",
        "    g = 1 - np.sum(probabilities**2)                     # Gini = 1 - sum(prob^2)\n",
        "    return g"
      ],
      "metadata": {
        "id": "71gU4C4INTTI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the Best Split"
      ],
      "metadata": {
        "id": "eBcWQ3xYca7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_numeric(datatype):\n",
        "    \"\"\" Determines if the feature is numeric(like age) or categorical(all the others). \"\"\"\n",
        "    return pd.api.types.is_numeric_dtype(datatype)\n",
        "\n",
        "def find_best_split(X, y):\n",
        "    \"\"\"\n",
        "    Inputs -\n",
        "    X: DataFrame of features, y: Series of labels\n",
        "    Returns: Details of best split\n",
        "    \"\"\"\n",
        "    best_feature = None\n",
        "    best_value = None\n",
        "    best_gini_split = float('inf')            # we minimize a loss function, so initially it is set to max\n",
        "    best_left_index = None\n",
        "    best_right_index = None\n",
        "\n",
        "    n_samples = len(y)\n",
        "\n",
        "    for feature in X.columns:                               # printing the feature will just print its name, ie., the columnname of the dataframe\n",
        "        unique_vals = X[feature].unique()\n",
        "\n",
        "        # Numeric feature\n",
        "        if is_numeric(X[feature].dtype):\n",
        "            sorted_vals = np.sort(unique_vals)\n",
        "\n",
        "            # Computing the gini for each unique value of the feature\n",
        "            for val in sorted_vals:\n",
        "                left_index = X[feature] <= val\n",
        "                right_index = X[feature] > val\n",
        "                if left_index.sum() == 0 or right_index.sum() == 0:\n",
        "                    continue\n",
        "\n",
        "                # Compute weighted Gini\n",
        "                left_gini = gini_impurity(y[left_index])\n",
        "                right_gini = gini_impurity(y[right_index])\n",
        "                w_left = left_index.sum() / n_samples\n",
        "                w_right = right_index.sum() / n_samples\n",
        "                gini_split = w_left * left_gini + w_right * right_gini\n",
        "\n",
        "                # Update best split\n",
        "                if gini_split < best_gini_split:\n",
        "                    best_gini_split = gini_split\n",
        "                    best_feature = feature\n",
        "                    best_value = val\n",
        "                    best_left_index = left_index\n",
        "                    best_right_index = right_index\n",
        "        else:\n",
        "            # Categorical feauter\n",
        "            for val in unique_vals:\n",
        "                left_index = X[feature] == val\n",
        "                right_index = X[feature] != val\n",
        "                if left_index.sum() == 0 or right_index.sum() == 0:\n",
        "                    continue\n",
        "\n",
        "                # Compute weighted Gini\n",
        "                left_gini = gini_impurity(y[left_index])\n",
        "                right_gini = gini_impurity(y[right_index])\n",
        "                w_left = left_index.sum() / n_samples\n",
        "                w_right = right_index.sum() / n_samples\n",
        "                gini_split = w_left * left_gini + w_right * right_gini\n",
        "\n",
        "                # Update best split\n",
        "                if gini_split < best_gini_split:\n",
        "                    best_gini_split = gini_split\n",
        "                    best_feature = feature\n",
        "                    best_value = val\n",
        "                    best_left_index = left_index\n",
        "                    best_right_index = right_index\n",
        "\n",
        "    return best_feature, best_value, best_gini_split, best_left_index, best_right_index"
      ],
      "metadata": {
        "id": "SCqxCaUtOnVu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Tree"
      ],
      "metadata": {
        "id": "z7_auGNPcoL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def majority_class(y):\n",
        "    \"\"\"Returns the most common label in y.\"\"\"\n",
        "    return y.value_counts().idxmax()\n",
        "\n",
        "def build_tree(X, y, depth=0, max_depth=5, min_samples_split=2):\n",
        "    if len(np.unique(y)) == 1:\n",
        "        return {\"leaf\": True, \"prediction\": y.iloc[0]}\n",
        "\n",
        "    if depth >= max_depth or len(y) < min_samples_split:\n",
        "        return {\"leaf\": True, \"prediction\": majority_class(y)}\n",
        "\n",
        "    # compute the best split\n",
        "    best_feature, best_value, best_gini_split, left_index, right_index = find_best_split(X, y)\n",
        "\n",
        "    # If there was no valid split, just return a leaf\n",
        "    if best_feature is None:\n",
        "        return {\"leaf\": True, \"prediction\": majority_class(y)}\n",
        "\n",
        "    node = {\n",
        "        \"leaf\": False,\n",
        "        \"feature\": best_feature,\n",
        "        \"value\": best_value,\n",
        "        \"gini\": best_gini_split\n",
        "    }\n",
        "\n",
        "    # Build subtrees\n",
        "    X_left, y_left = X[left_index], y[left_index]\n",
        "    X_right, y_right = X[right_index], y[right_index]\n",
        "\n",
        "    node[\"left\"] = build_tree(X_left, y_left, depth + 1, max_depth, min_samples_split)\n",
        "    node[\"right\"] = build_tree(X_right, y_right, depth + 1, max_depth, min_samples_split)\n",
        "\n",
        "    return node\n"
      ],
      "metadata": {
        "id": "EnebAnN9PUMC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method for Predicting new Test Cases"
      ],
      "metadata": {
        "id": "U2XPsIaicyHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sample(sample, tree):\n",
        "    \"\"\"\n",
        "    sample: a single row of features. can be a dict, or pd.series.\n",
        "    tree: dictionary for the decision tree\n",
        "    \"\"\"\n",
        "    # If it's a leaf node, return the stored prediction\n",
        "    if tree[\"leaf\"]:\n",
        "        return tree[\"prediction\"]\n",
        "\n",
        "    feature = tree[\"feature\"]\n",
        "    value = tree[\"value\"]\n",
        "\n",
        "    if is_numeric(value):\n",
        "        # Numeric split\n",
        "        if sample[feature] <= value:\n",
        "            return predict_sample(sample, tree[\"left\"])\n",
        "        else:\n",
        "            return predict_sample(sample, tree[\"right\"])\n",
        "    else:\n",
        "        # Categorical split\n",
        "        if sample[feature] == value:\n",
        "            return predict_sample(sample, tree[\"left\"])\n",
        "        else:\n",
        "            return predict_sample(sample, tree[\"right\"])\n"
      ],
      "metadata": {
        "id": "WSad0yLfYNLJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Decision Tree"
      ],
      "metadata": {
        "id": "W-hU1CT-c5n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "import pprint\n",
        "\n",
        "# Buy is our Label. Classes: Yes and No\n",
        "X = df.drop(\"Buy\", axis=1)\n",
        "y = df[\"Buy\"]\n",
        "\n",
        "# Build the tree\n",
        "tree = build_tree(X, y, max_depth = 5, min_samples_split = 2)\n",
        "\n",
        "pprint.pprint(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mViuGr37anKl",
        "outputId": "1d13ad71-67ee-49b7-d1ab-a18bc45d8e4b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'feature': 'Income',\n",
            " 'gini': np.float64(0.1999999999999999),\n",
            " 'leaf': False,\n",
            " 'left': {'leaf': True, 'prediction': 'No'},\n",
            " 'right': {'feature': 'Age',\n",
            "           'gini': np.float64(0.2),\n",
            "           'leaf': False,\n",
            "           'left': {'leaf': True, 'prediction': 'Yes'},\n",
            "           'right': {'feature': 'Age',\n",
            "                     'gini': np.float64(0.0),\n",
            "                     'leaf': False,\n",
            "                     'left': {'leaf': True, 'prediction': 'No'},\n",
            "                     'right': {'leaf': True, 'prediction': 'Yes'},\n",
            "                     'value': np.int64(50)},\n",
            "           'value': np.int64(45)},\n",
            " 'value': 'High'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifying the Test Case"
      ],
      "metadata": {
        "id": "74MtIF1IdCMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = [{\"Age\": 42, \"Income\": \"Low\", \"Student\": \"No\", \"Credit\": \"Excellent\"}]\n",
        "test_case = pd.DataFrame(new_data)\n",
        "\n",
        "prediction = predict_sample(test_case.iloc[0], tree)\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3rCEBW7b6j8",
        "outputId": "8d43ee96-5dd3-4297-cce9-b9e447164725"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 a - Bagging"
      ],
      "metadata": {
        "id": "uBWPF_Kve7Ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bagging_dataset(X, y):\n",
        "    \"\"\"\n",
        "    Creates a bagging or bootstrap sample of the same size as X,y\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    indices = np.random.randint(0, n, size=n)           # random indices with replacement\n",
        "    X_bag = X.iloc[indices].reset_index(drop=True)\n",
        "    y_bag = y.iloc[indices].reset_index(drop=True)\n",
        "\n",
        "    sampled_set = set(indices)\n",
        "    all_indices = set(range(n))\n",
        "    oob_indices = np.array(list(all_indices - sampled_set))\n",
        "\n",
        "    return X_bag, y_bag, oob_indices\n",
        "\n",
        "def bagging_ensemble(X, y, n_trees=10, max_depth=5, min_samples_split=2):\n",
        "    \"\"\"\n",
        "    Building an ensemble(here, 10) of decision trees via Bagging.\n",
        "    Returns: trees and corresponding oob_lists\n",
        "    \"\"\"\n",
        "    trees = []\n",
        "    oob_lists = []\n",
        "\n",
        "    for _ in range(n_trees):\n",
        "        X_bag, y_bag, oob_indices = generate_bagging_dataset(X, y)\n",
        "\n",
        "        tree = build_tree(X_bag, y_bag, depth=0, max_depth=max_depth,\n",
        "                          min_samples_split=min_samples_split)\n",
        "\n",
        "        trees.append(tree)\n",
        "        oob_lists.append(oob_indices)\n",
        "\n",
        "    return trees, oob_lists\n"
      ],
      "metadata": {
        "id": "J00BTotAd3bD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_oob_error(X, y, trees, oob_lists):\n",
        "    n = len(X)\n",
        "    correct_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        # Finding trees that are OOB for sample i\n",
        "        oob_tree_indices = []\n",
        "        for t_idx, oob_idx_array in enumerate(oob_lists):\n",
        "            if i in oob_idx_array:\n",
        "                oob_tree_indices.append(t_idx)\n",
        "\n",
        "        if len(oob_tree_indices) == 0:\n",
        "            continue\n",
        "\n",
        "\n",
        "        votes = []\n",
        "        for t_idx in oob_tree_indices:\n",
        "            tree = trees[t_idx]\n",
        "            pred = predict_sample(X.iloc[i], tree)\n",
        "            votes.append(pred)\n",
        "\n",
        "        # Majority vote\n",
        "        votes_series = pd.Series(votes)\n",
        "        majority_pred = votes_series.value_counts().idxmax()\n",
        "\n",
        "        if majority_pred == y.iloc[i]:\n",
        "            correct_count += 1\n",
        "        total_count += 1\n",
        "\n",
        "    if total_count == 0:\n",
        "        return None\n",
        "\n",
        "    oob_error = 1 - (correct_count / total_count)\n",
        "    return oob_error\n"
      ],
      "metadata": {
        "id": "WZy-JPKLsSvH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Trees and Computing the OOB Error for Bagging\n",
        "Upon trying multiple times, the best OOB Error for bagging was 0.375."
      ],
      "metadata": {
        "id": "j1TCx65Pv2TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trees_bag, oob_lists_bag = bagging_ensemble(X, y, n_trees=10, max_depth=5, min_samples_split=2)\n",
        "\n",
        "oob_error_bag = compute_oob_error(X, y, trees_bag, oob_lists_bag)\n",
        "print(\"Bagging OOB error:\", oob_error_bag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8eY3N52vsL8",
        "outputId": "a3c18f07-b99b-4d67-dda4-19f14fe0f543"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging OOB error: 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 b - Random Forest"
      ],
      "metadata": {
        "id": "P1gKDreRxJMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# both functions are basically the same as the ones in Q3, only difference is that I am selecting a random subset of features to split\n",
        "def find_best_split_random_features(X, y, max_features=2):\n",
        "    \"\"\"\n",
        "    Modification from find_best_split: we use a random subset of features.\n",
        "    \"\"\"\n",
        "    all_features = list(X.columns)\n",
        "    if len(all_features) <= max_features:\n",
        "        features_to_consider = all_features\n",
        "    else:\n",
        "        features_to_consider = random.sample(all_features, max_features)\n",
        "\n",
        "    best_feature = None\n",
        "    best_value = None\n",
        "    best_gini_split = float('inf')\n",
        "    best_left_index = None\n",
        "    best_right_index = None\n",
        "\n",
        "    n_samples = len(y)\n",
        "\n",
        "    for feature in features_to_consider:\n",
        "        unique_vals = X[feature].unique()\n",
        "\n",
        "        if is_numeric(X[feature].dtype):\n",
        "            sorted_vals = np.sort(unique_vals)\n",
        "            for val in sorted_vals:\n",
        "                left_index = X[feature] <= val\n",
        "                right_index = X[feature] > val\n",
        "                if left_index.sum() == 0 or right_index.sum() == 0:\n",
        "                    continue\n",
        "\n",
        "                left_gini = gini_impurity(y[left_index])\n",
        "                right_gini = gini_impurity(y[right_index])\n",
        "                w_left = left_index.sum() / n_samples\n",
        "                w_right = right_index.sum() / n_samples\n",
        "                gini_split = w_left * left_gini + w_right * right_gini\n",
        "\n",
        "                if gini_split < best_gini_split:\n",
        "                    best_gini_split = gini_split\n",
        "                    best_feature = feature\n",
        "                    best_value = val\n",
        "                    best_left_index = left_index\n",
        "                    best_right_index = right_index\n",
        "        else:\n",
        "            for val in unique_vals:\n",
        "                left_index = X[feature] == val\n",
        "                right_index = X[feature] != val\n",
        "                if left_index.sum() == 0 or right_index.sum() == 0:\n",
        "                    continue\n",
        "\n",
        "                left_gini = gini_impurity(y[left_index])\n",
        "                right_gini = gini_impurity(y[right_index])\n",
        "                w_left = left_index.sum() / n_samples\n",
        "                w_right = right_index.sum() / n_samples\n",
        "                gini_split = w_left * left_gini + w_right * right_gini\n",
        "\n",
        "                if gini_split < best_gini_split:\n",
        "                    best_gini_split = gini_split\n",
        "                    best_feature = feature\n",
        "                    best_value = val\n",
        "                    best_left_index = left_index\n",
        "                    best_right_index = right_index\n",
        "\n",
        "    return best_feature, best_value, best_gini_split, best_left_index, best_right_index\n",
        "\n",
        "\n",
        "def build_tree_random_forest(X, y, depth=0, max_depth=5, min_samples_split=2, max_features=2):\n",
        "    if len(np.unique(y)) == 1:\n",
        "        return {\"leaf\": True, \"prediction\": y.iloc[0]}\n",
        "    if depth >= max_depth or len(y) < min_samples_split:\n",
        "        return {\"leaf\": True, \"prediction\": majority_class(y)}\n",
        "\n",
        "    bf, bv, bg, left_idx, right_idx = find_best_split_random_features(X, y, max_features=max_features)\n",
        "    if bf is None:\n",
        "        return {\"leaf\": True, \"prediction\": majority_class(y)}\n",
        "\n",
        "    node = {\n",
        "        \"leaf\": False,\n",
        "        \"feature\": bf,\n",
        "        \"value\": bv,\n",
        "        \"gini\": bg\n",
        "    }\n",
        "\n",
        "    X_left, y_left = X[left_idx], y[left_idx]\n",
        "    X_right, y_right = X[right_idx], y[right_idx]\n",
        "\n",
        "    node[\"left\"] = build_tree_random_forest(X_left, y_left, depth+1, max_depth, min_samples_split, max_features)\n",
        "    node[\"right\"] = build_tree_random_forest(X_right, y_right, depth+1, max_depth, min_samples_split, max_features)\n",
        "\n",
        "    return node\n"
      ],
      "metadata": {
        "id": "9kaCrlxUwRC7"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_forest_ensemble(X, y, n_trees=10, max_depth=5, min_samples_split=2, max_features=2):\n",
        "    \"\"\"\n",
        "    Build a Random Forest\n",
        "    \"\"\"\n",
        "    trees = []\n",
        "    oob_lists = []\n",
        "\n",
        "    for _ in range(n_trees):\n",
        "        X_bag, y_bag, oob_indices = generate_bagging_dataset(X, y)\n",
        "\n",
        "        tree = build_tree_random_forest(X_bag, y_bag, depth=0, max_depth=max_depth, min_samples_split=min_samples_split, max_features=max_features)\n",
        "\n",
        "        trees.append(tree)\n",
        "        oob_lists.append(oob_indices)\n",
        "\n",
        "    return trees, oob_lists\n"
      ],
      "metadata": {
        "id": "_D7fTB2qyh6O"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Trees and Computing the OOB Error for Random Forest\n",
        "Upon trying multiple times, the best OOB Error for RF was 0.25."
      ],
      "metadata": {
        "id": "MsSzR7ajzTwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trees_rf, oob_lists_rf = random_forest_ensemble(X, y, n_trees=10, max_depth=5, min_samples_split=2, max_features=2)\n",
        "\n",
        "oob_error_rf = compute_oob_error(X, y, trees_rf, oob_lists_rf)\n",
        "print(\"Random Forest OOB error:\", oob_error_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSWiFwi7y_k0",
        "outputId": "1825735c-230b-4a7a-9c6f-d8f71880d553"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest OOB error: 0.25\n"
          ]
        }
      ]
    }
  ]
}
